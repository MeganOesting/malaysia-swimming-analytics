### How this can become a PhD in Educational Measurement

- A defensible dissertation could center on designing, validating, and implementing a measurement framework that:
  - Creates regionally-relevant performance scales (Malaysia Age Points).
  - Sets event- and age-specific "On Track" standards predictive of SEA Games podium outcomes.
  - Builds and validates an evaluation rubric for talent identification and investment decisions (Pelapis).
- This is squarely Educational Measurement: construct definition, test design, standard setting, reliability, validity evidence, fairness, and consequential impact.

### A possible dissertation structure (3–4-study model)

- Study 1 — Construct definition and scale design (Malaysia Age Points)
  - Define the construct: age- and event-adjusted swimming performance.
  - Build the scale: calibrate MAP using AQUA/USA distributions; verify monotonicity, interpretability, and invariance across age, gender, events.
  - Evidence: internal structure (factor/scale coherence), relations to other variables (AQUA, ranks), fairness (DIF across groups), reliability (precision/SEM across ability range).

- Study 2 — Regional standard setting for On Track benchmarks
  - Problem: endpoint moves from world A to SEA bronze; naive back-mapping reduces predictiveness at ages 15–18.
  - Method: event/gender/age-specific z-based targets on reference distributions; standard-setting methods (Angoff/Bookmark variants) adapted to continuous performance metrics; sensitivity analyses for different selectivity levels.
  - Evidence: predictive validity (probability of future SEA podium/selection), external relations (national rankings), consequential validity (selection yield/cost trade-offs).

- Study 3 — Evaluation rubric development for TI decisions
  - Instrument: a decision rubric combining performance, progression, resilience, training context, etc.
  - Methods: instrument design; rater training; reliability via Generalizability Theory and/or Many-Facet Rasch Measurement (MFRM) for raters×events×age.
  - Evidence: inter-rater reliability, dimensionality, fairness (rater severity bias), decision consistency.

- Study 4 — Longitudinal and causal-inference evaluation
  - Model retention and progression (ages 15–18) and outcomes (SEA podium/qualifying).
  - Methods: multilevel longitudinal models, survival/attrition models, Bayesian hierarchical updates; policy simulation of funding scenarios.
  - Evidence: incremental validity (rubric + On Track improves prediction vs performance alone); robustness across cohorts.

### Core validity framework to satisfy a PhD defense

- Adopt Kane's argument-based validation (or Messick's unified validity) and the Standards for Educational and Psychological Testing (AERA/APA/NCME).
- Required lines of evidence:
  - Test content: expert alignment of MAP/On Track definitions with the talent construct.
  - Response process: how coaches/analysts interpret and use the rubric and thresholds.
  - Internal structure: dimensionality, invariance (age, gender, event), scale properties.
  - Relations to other variables: concurrent/predictive validity (national results, SEA outcomes).
  - Consequences: intended vs unintended impacts (equity, resource allocation, retention).
- Fairness and accessibility: DIF/DBF checks by gender, state/club context; transparent accommodations for less-resourced contexts.

### Methods toolbox that fits Educational Measurement

- Scale design: item/score transformations, log-time or AQUA scaling, standard errors across range.
- Standard setting: Bookmark on ordered exemplars, Angoff on performance descriptors, policy-driven cut-score optimization with uncertainty.
- Reliability: Cronbach (if composite), G-Theory for multi-facet decisions, MFRM for rater/event/age facets.
- Validity statistics: correlation, ROC/PR curves, decision consistency/inconsistency, calibration plots, uplift modeling.
- Longitudinal: mixed-effects growth models; survival analysis for dropout; Bayesian updating for small samples.
- Fairness: DIF/MIMIC/IRT, subgroup sensitivity, distributional equity analyses.

### Data and design requirements

- Datasets:
  - Historical Malaysian performances by event/gender/age; SEA Games medal ages/times.
  - USA distributions (by age/event/gender) to norm z-based thresholds.
  - Pelapis selection/funding records and outcomes for predictive checks.
  - Rater rubric scores with multiple raters/events for G/MFRM.
- Design:
  - Pre-register analysis plan; hold-out and temporal validation; sensitivity and uncertainty quantification.
  - Expert panels for content validity and standard setting.
  - Pilot and field tests for the rubric; rater training protocol.

### Ethics and governance

- IRB/ethics approval, consent where applicable, de-identification, secure storage.
- Bias mitigation plans; transparent reporting of trade-offs (false positives/negatives).
- Stakeholder review (national federation, coaches) to assess consequences.

### What dissertations generally need for a successful defense

- A clear problem statement and theoretical framework grounded in measurement science.
- Original contribution: new scale, adapted standard-setting for continuous sport metrics, validated rubric, and a validated predictive selection system for a regional context.
- Rigorous methodology with appropriate validity evidence and reliability analyses.
- Transparent limitations, uncertainty, and reproducibility (code, data-sharing plan).
- Practical and theoretical implications, plus a path for future research and policy translation.

### Deliverables you'd produce

- Malaysia Age Points technical manual (construct, methods, reliability, validity).
- On Track standard-setting report per event/gender with policy-ready thresholds.
- Evaluation rubric manual (training, scoring, reliability, fairness).
- Longitudinal validation report with predictive performance, calibration, and simulation of funding strategies.
- Reproducible code and a de-identified data appendix.

In short: frame your current work as the design, validation, and implementation of a measurement system for talent identification. Anchor it in recognized validity frameworks, produce multi-study evidence, and demonstrate predictive, fair, and consequentially positive decisions for Malaysia's pathway.
